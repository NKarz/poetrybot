{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STi3uF-O3z7f",
    "outputId": "70c6ac9f-8883-449c-d71c-e924d4fe604e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.5.12)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kaggle) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kaggle) (2.24.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kaggle) (2020.6.20)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kaggle) (4.49.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqiPnC384rfk",
    "outputId": "934bb656-eafa-4855-9609-808c1fc5c4fb"
   },
   "outputs": [],
   "source": [
    "# !mkdir .kaggle\n",
    "# !copy kaggle.json .kaggle\n",
    "# !chmod 600 .kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kExlttt44GC",
    "outputId": "e5f0a596-d2cb-494e-fa0f-fe8d195ec12f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete-poetryfoundationorg-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download johnhallman/complete-poetryfoundationorg-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00pd8He05GWD",
    "outputId": "236a789f-1627-43db-8df4-8d4c902baa10"
   },
   "outputs": [],
   "source": [
    "# !unzip complete-poetryfoundationorg-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzMWiuHE7FDF",
    "outputId": "6c53fe03-6a68-49eb-d98f-e4c7ba9860e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-global-vectors-for-word-representation.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download rtatman/glove-global-vectors-for-word-representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzrBIwFT7JO0",
    "outputId": "6d59a32e-3133-4deb-9b1c-9977f8a3dd49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! unzip glove-global-vectors-for-word-representation.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2x19UDo53eA5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import spatial\n",
    "from collections import Counter\n",
    "\n",
    "ds_path = \"./complete-poetryfoundationorg-dataset/kaggle_poem_dataset.csv\"\n",
    "glove_path = \"./glove-global-vectors-for-word-representation/glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.21.4)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (41.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gTp33hVA3sLo"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, GRU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9r2rr7Yl3wgb",
    "outputId": "1a6af6df-2adc-45e8-92b2-9d9b1cda19cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poetry Foundation ID</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>!</td>\n",
       "      <td>55489</td>\n",
       "      <td>Dear Writers, I’m compiling the first in what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>0</td>\n",
       "      <td>41729</td>\n",
       "      <td>Philosophic\\nin its complex, ovoid emptiness,\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>1-800-FEAR</td>\n",
       "      <td>57135</td>\n",
       "      <td>We'd  like  to  talk  with  you  about  fear t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Joseph Brodsky</td>\n",
       "      <td>1 January 1965</td>\n",
       "      <td>56736</td>\n",
       "      <td>The Wise Men will unlearn your name.\\nAbove yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ted Berrigan</td>\n",
       "      <td>3 Pages</td>\n",
       "      <td>51624</td>\n",
       "      <td>For Jack Collom\\n10 Things I do Every Day\\n\\np...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             Author           Title  Poetry Foundation ID  \\\n",
       "0           0     Wendy Videlock               !                 55489   \n",
       "1           1  Hailey Leithauser               0                 41729   \n",
       "2           2      Jody Gladding      1-800-FEAR                 57135   \n",
       "3           3     Joseph Brodsky  1 January 1965                 56736   \n",
       "4           4       Ted Berrigan         3 Pages                 51624   \n",
       "\n",
       "                                             Content  \n",
       "0  Dear Writers, I’m compiling the first in what ...  \n",
       "1  Philosophic\\nin its complex, ovoid emptiness,\\...  \n",
       "2  We'd  like  to  talk  with  you  about  fear t...  \n",
       "3  The Wise Men will unlearn your name.\\nAbove yo...  \n",
       "4  For Jack Collom\\n10 Things I do Every Day\\n\\np...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_df = pd.read_csv(ds_path)\n",
    "poems_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "So6Oc9RU3-i9",
    "outputId": "013aff7d-26bb-421e-9877-089453a62b6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Author</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>William Shakespeare</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alfred, Lord Tennyson</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rae Armantrout</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>William Wordsworth</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Content\n",
       "Author                        \n",
       "William Shakespeare         85\n",
       "Anonymous                   82\n",
       "Alfred, Lord Tennyson       78\n",
       "Rae Armantrout              62\n",
       "William Wordsworth          59"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_df.groupby(\"Author\").agg({\"Content\": \"count\"}).sort_values(\"Content\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IXgbLbP5n5C",
    "outputId": "8d224989-bc82-4273-d78f-a4b918706fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line examples: \n",
      "['Break, break, break,', 'On thy cold gray stones, O Sea!', 'And I would that my tongue could utter', 'The thoughts that arise in me.', '', \"O, well for the fisherman's boy,\", 'That he shouts with his sister at play!', 'O, well for the sailor lad,', 'That he sings in his boat on the bay!', '']\n"
     ]
    }
   ],
   "source": [
    "#submission is from Alfred, Lord Tennyson, for his semi-modern poetry\n",
    "anonymous_poems = (poems_df[poems_df[\"Author\"] == \"Alfred, Lord Tennyson\"])\n",
    "print(\"Line examples: \")\n",
    "print(anonymous_poems.iloc[0,4].split('\\n')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2ZsLgLT5ysB",
    "outputId": "60990db0-69f3-4de9-e4f3-236a4bcc1d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  186569\n"
     ]
    }
   ],
   "source": [
    "poems_combined = \"\\n\".join(anonymous_poems.iloc[:,4].values)\n",
    "print(\"Total number of characters: \", len(poems_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcmN5Hf56bjd",
    "outputId": "4091702a-8bf6-4810-ef15-5201b9d76072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines:  5246\n"
     ]
    }
   ],
   "source": [
    "poem_lines = poems_combined.split('\\n')\n",
    "print(\"Number of lines: \", len(poem_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kytFD3Bg6gGQ"
   },
   "outputs": [],
   "source": [
    "# prepare the input and target lines\n",
    "input_lines = [\"<sos> \"+line for line in poem_lines] # in each of the input we add <sos> token idicating the begining of a line\n",
    "target_lines = [line+ \" <eos>\" for line in poem_lines] # while target lines are appended with with <eos> token indicating end of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "m770Tke36344"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 250 # number of times the model is trained on the entire training dataset\n",
    "BATCH_SIZE = 64 # number of data points to consider to train at a single point of time\n",
    "LATENT_DIM = 200 # the size of the hidden state/vector\n",
    "EMBEDDING_DIM = 200 # size of the word embeddings \n",
    "MAX_VOCAB_SIZE = 30000 # the maximum number of words to consider\n",
    "VALIDATION_SPLIT = 0.2 # % of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5-Oa_cj65fu",
    "outputId": "dca1ea3c-f0ef-4789-8061-b65e66a904af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st input sequence:  [  1 742 742 742   0   0   0   0   0   0   0   0]\n",
      "1st target sequence:  [742 742 742   2   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "class SequenceGenerator():\n",
    "    \n",
    "    # takes as input an input and output sequence\n",
    "    def __init__(self, input_lines, target_lines, max_seq_len=None, max_vocab_size=10000, embedding_dim=200):        \n",
    "        self.input_lines = input_lines\n",
    "        self.target_lines = target_lines\n",
    "        \n",
    "        self.MAX_SEQ_LEN = max_seq_len\n",
    "        self.MAX_VOCAB_SIZE = max_vocab_size\n",
    "        self.EMBEDDING_DIM = embedding_dim\n",
    "    \n",
    "    \n",
    "    def initialize_embeddings(self):\n",
    "        \"\"\"Reads the GloVe word-embeddings and creates embedding matrix and word to index and index to word mapping.\"\"\"\n",
    "        \n",
    "        # load the word embeddings\n",
    "        self.word2vec = {}\n",
    "        with open(glove_path, 'r', encoding=\"utf8\") as file:\n",
    "            for line in file:\n",
    "                vectors = line.split()\n",
    "                self.word2vec[vectors[0]] = np.asarray(vectors[1:], dtype=\"float32\")\n",
    "                \n",
    "        # get the embeddings matrix\n",
    "        self.num_words = min(self.MAX_VOCAB_SIZE, len(self.word2idx)+1)\n",
    "        self.embeddings_matrix = np.zeros((self.num_words, self.EMBEDDING_DIM))\n",
    "        \n",
    "        for word, idx in self.word2idx.items():\n",
    "            if idx <= self.num_words:\n",
    "                word_embeddings = self.word2vec.get(word)\n",
    "                if word_embeddings is not None:\n",
    "                    self.embeddings_matrix[idx] = word_embeddings\n",
    "                    \n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "    \n",
    "    \n",
    "    def prepare_sequences(self, filters=''):\n",
    "        # train the tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, filters='')\n",
    "        self.tokenizer.fit_on_texts(self.input_lines+self.target_lines)\n",
    "        \n",
    "        # get the word-index mapping and initialize embeddings\n",
    "        self.word2idx = self.tokenizer.word_index\n",
    "        self.initialize_embeddings()\n",
    "        \n",
    "        # tokenize the input and target lines\n",
    "        self.input_sequences = self.tokenizer.texts_to_sequences(self.input_lines)\n",
    "        self.target_sequences = self.tokenizer.texts_to_sequences(self.target_lines)\n",
    "        \n",
    "        # get the max sequence len from the data\n",
    "        max_seq_len = max(list(map(len, self.input_lines+self.target_lines)))\n",
    "        if self.MAX_SEQ_LEN:\n",
    "            self.MAX_SEQ_LEN = min(self.MAX_SEQ_LEN, max_seq_len)\n",
    "        else:\n",
    "            self.MAX_SEQ_LEN = max_seq_len\n",
    "            \n",
    "        # pad the sequences\n",
    "        self.input_sequences = pad_sequences(self.input_sequences, maxlen=self.MAX_SEQ_LEN, padding=\"post\")\n",
    "        self.target_sequences = pad_sequences(self.target_sequences, maxlen=self.MAX_SEQ_LEN, padding=\"post\")\n",
    "        \n",
    "        print(\"1st input sequence: \", self.input_sequences[0])\n",
    "        print(\"1st target sequence: \", self.target_sequences[0])\n",
    "        \n",
    "        \n",
    "    def one_hot_encoding(self):\n",
    "        \n",
    "        # it will be a 3 dimensional array where\n",
    "        # first-dim is the number of target lines\n",
    "        # second-dim is the size of the sequences\n",
    "        # third-dim is the number of words in the dataset\n",
    "        self.one_hot_targets = np.zeros((len(self.target_sequences), self.MAX_SEQ_LEN, self.num_words))\n",
    "        \n",
    "        for seq_idx, seq in enumerate(self.target_sequences):\n",
    "            for word_idx, word_id in enumerate(self.target_sequences[seq_idx]):\n",
    "                if word_id > 0:\n",
    "                    self.one_hot_targets[seq_idx, word_idx, word_id] = 1\n",
    "    \n",
    "    \n",
    "    def get_closest_word(self, word_vec):\n",
    "        max_dist = 9999999999\n",
    "        closest_word = \"NULL\"\n",
    "        \n",
    "        # iterate overall the words and find the closest one\n",
    "        for word, vec in self.word2vec.items():\n",
    "            \n",
    "            # get the cosine distance between the words\n",
    "            dist = spatial.distance.cosine(word_vec, vec)\n",
    "            \n",
    "            # compare the distance and keep the minimum\n",
    "            if dist < max_dist:\n",
    "                max_dist = dist\n",
    "                closest_word = word\n",
    "        \n",
    "        return closest_word\n",
    "\n",
    "\n",
    "# create an object of the class\n",
    "sg_obj = SequenceGenerator(input_lines, target_lines, max_seq_len=12, \n",
    "                           max_vocab_size=MAX_VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# prepare the input & target sequences\n",
    "sg_obj.prepare_sequences()\n",
    "# create the One-hot encoding on the target sequences\n",
    "sg_obj.one_hot_encoding()\n",
    "\n",
    "# make sure the tokenized words contains <sos> & <eos>\n",
    "assert '<sos>' in sg_obj.word2idx\n",
    "assert '<eos>' in sg_obj.word2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7SIZq60v694f"
   },
   "outputs": [],
   "source": [
    "# add the embedding layer\n",
    "# weights: is the embedding_matrix we created in the SequenceGenerator class\n",
    "embedding = Embedding(\n",
    "    input_dim=sg_obj.num_words,\n",
    "    output_dim=sg_obj.EMBEDDING_DIM,\n",
    "    weights=[sg_obj.embeddings_matrix]\n",
    ")\n",
    "\n",
    "state_h = Input(shape=(LATENT_DIM,)) #hidden state\n",
    "state_c = Input(shape=(LATENT_DIM,))#cell state\n",
    "\n",
    "sequence_input = Input(shape=(sg_obj.MAX_SEQ_LEN,))\n",
    "\n",
    "# the below layer gets the embeddings for the words in the sequence\n",
    "embedding_ = embedding(sequence_input)\n",
    "\n",
    "lstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True)\n",
    "\n",
    "x, h_, c_ = lstm(embedding_, initial_state=[state_h, state_c])\n",
    "\n",
    "dense = Dense(sg_obj.num_words, activation=\"softmax\")\n",
    "output = dense(x)\n",
    "\n",
    "Encoder = Model([sequence_input, state_h, state_c], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MllGHr9J8Kcm"
   },
   "outputs": [],
   "source": [
    "deco_inp = Input(shape=(1,))\n",
    "\n",
    "deco_embed = embedding(deco_inp)\n",
    "\n",
    "deco_x, h, c = lstm(deco_embed, initial_state=[state_h, state_c])\n",
    "deco_output = dense(deco_x)\n",
    "\n",
    "Decoder = Model([deco_inp, state_h, state_c], [deco_output, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Q83k1PVF-dRd"
   },
   "outputs": [],
   "source": [
    "Encoder.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# initial hidden/cell state vector containing all zeros\n",
    "# this will be passed into the LSTM model\n",
    "initial_state = np.zeros((len(sg_obj.input_sequences), LATENT_DIM))\n",
    "\n",
    "# train the model\n",
    "history = Encoder.fit(\n",
    "    [sg_obj.input_sequences, initial_state, initial_state], # pass the input sequences and the state vectors\n",
    "    sg_obj.one_hot_targets, # the one-hot encoding of the target sequences\n",
    "    batch_size=BATCH_SIZE, # the batch size\n",
    "    epochs=EPOCHS, # number of times to train the model\n",
    "    validation_split=VALIDATION_SPLIT, # % of data for validation\n",
    "    verbose=0 # to suppress the information printed for each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SKrp7IsS-fMn"
   },
   "outputs": [],
   "source": [
    "def get_context(sequences, query_word):\n",
    "    assert query_word in sg_obj.word2idx\n",
    "    \n",
    "    # null vector containing all zeroes\n",
    "    query_word_embed = sg_obj.word2vec.get(query_word, np.zeros(shape=(EMBEDDING_DIM)))\n",
    "    \n",
    "    if sequences == []:\n",
    "        return query_word_embed\n",
    "    \n",
    "    # to keep all the sentence embeddings\n",
    "    seq_embeddings = []\n",
    "    for seq in sequences:\n",
    "        \n",
    "        # add up all the word embeddings of a sequence\n",
    "        zero_vector = np.zeros(shape=(EMBEDDING_DIM))\n",
    "        for word in seq:\n",
    "            zero_vector += sg_obj.word2vec.get(word, np.zeros(shape=(EMBEDDING_DIM)))\n",
    "            \n",
    "        seq_embeddings.append(zero_vector)\n",
    "    seq_embeddings = np.array(seq_embeddings)\n",
    "            \n",
    "    weights = []\n",
    "    for seq_embed in seq_embeddings:\n",
    "        # get the distance between the query word and the sentence embeddings\n",
    "        dist = spatial.distance.cosine(seq_embed, query_word_embed)\n",
    "        weights.append(np.array([dist]))\n",
    "        \n",
    "    # normalize the distances\n",
    "    weights = np.array(weights/max(weights))\n",
    "        \n",
    "    # get the final weighted context\n",
    "    context = sum(weights * seq_embeddings)\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HbLa57ro-k2E"
   },
   "outputs": [],
   "source": [
    "def get_sample_line(context):\n",
    "    # sentence start token\n",
    "    sos_token = np.array([[sg_obj.word2idx.get(\"<sos>\")]])\n",
    "    \n",
    "    # create the empty lstm state vectors\n",
    "    h = np.array([context])    \n",
    "    c = np.zeros(shape=(1, LATENT_DIM))\n",
    "    \n",
    "    # so we know when to quit\n",
    "    eos_token = sg_obj.word2idx['<eos>']\n",
    "    \n",
    "    output_sequence = []\n",
    "    \n",
    "    # limit the length of the generated line\n",
    "    for i in range(sg_obj.MAX_SEQ_LEN):\n",
    "        \n",
    "        # predict the first word\n",
    "        # the outputed stated are passed to the lstm to generate the next word in the sequence\n",
    "        o, h, c = Decoder.predict([sos_token, h, c])\n",
    "        \n",
    "        # get the probabilities generated from the dense layer\n",
    "        probs = o[0,0]\n",
    "        \n",
    "        if np.argmax(probs) ==0:\n",
    "            print(\"Something went wrong!!\")\n",
    "        \n",
    "        probs = np.nan_to_num(probs)\n",
    "        # the word-indices starts from 1 so 1st value does not count\n",
    "        probs[0] = 0 \n",
    "        \n",
    "        # normalize the probabilities\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # select a random word with provided probability of being selected\n",
    "        selected_idx = np.random.choice(len(probs), p=probs)\n",
    "        \n",
    "        # if the generated word is equal to eos_token, terminate\n",
    "        if selected_idx == eos_token:\n",
    "            break\n",
    "        \n",
    "        # append the generated word to the output_sequence\n",
    "        output_sequence.append(sg_obj.idx2word.get(selected_idx, \"Error <%d>\" % selected_idx))\n",
    "        \n",
    "        # the word generated will be used as an input to generated the new word\n",
    "        sos_token[0][0] = selected_idx\n",
    "    \n",
    "    # return the sequence\n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHPu-cDqJSmM",
    "outputId": "4adcf3df-0160-4b59-cbc9-d4f7d1d8776e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong!!\n",
      "Something went wrong!!\n",
      "Something went wrong!!\n",
      "Something went wrong!!\n",
      "\n",
      "\n",
      "\n",
      "and die.\n",
      "but want and weeping with sharp a moaning him, roar'd\n",
      "myriads but it? with toil,\n",
      "among with me?\"\n",
      "'i as came dangling by, was ringing to sport of shalott.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# the theme of the poem - only single word (for simplicity)\n",
    "query_word = \"love\"\n",
    "\n",
    "# to append the generated poem lines\n",
    "poem_lines = []\n",
    "\n",
    "# first sequence containing only ones, this will be used to generate the context\n",
    "sequences = []\n",
    "\n",
    "# we will be generating 8 lines, you can play around with this\n",
    "for line_no in range(10):\n",
    "    \n",
    "    # get the context, for the first line the context will contain the embeddings of the theme words itself\n",
    "    context = get_context(sequences, query_word)\n",
    "    \n",
    "    try:\n",
    "        # generate a new line and append it\n",
    "        sequences.append(get_sample_line(context))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    poem_lines.append(\" \".join(sequences[-1]))\n",
    "    \n",
    "print(\"\\n\\n\")\n",
    "print(\"\\n\".join(poem_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXfsy4k-JU_k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Poetry Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
